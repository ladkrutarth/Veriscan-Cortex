# GraphGuard ‚Äî AI-Powered Fraud Detection & Dynamic Authentication System

> **Course:** CS 5588 ‚Äî Data Science Capstone | **Date:** February 2026

---

## üìã Table of Contents
- [Project Overview](#project-overview)
- [System Architecture](#system-architecture)
- [Pipeline Workflow](#pipeline-workflow)
- [Repository Structure](#repository-structure)
- [Quick Start](#quick-start)
- [Component Details](#component-details)
- [Monitoring & Logging](#monitoring--logging)
- [Implemented Extensions](#implemented-extensions)
- [Demo Video](#demo-video)

---

## Project Overview

GraphGuard is an end-to-end **fraud detection and dynamic authentication system** that processes financial transaction data through a complete data pipeline:

**Data Sources ‚Üí Cloud Warehouse (Snowflake) ‚Üí Feature Engineering ‚Üí Modeling / Decision Layer ‚Üí Streamlit Dashboard**

The system uses a hybrid approach combining **rule-based heuristics**, **statistical anomaly detection** (z-score, velocity checks), and **machine learning** (IsolationForest) to score transaction risk in real-time. An AI-powered authentication module uses **Google Gemini LLM** with **RAG (Retrieval-Augmented Generation)** over project data to dynamically generate security challenges adapted to each user's risk profile.

### Key Capabilities
- **3,000 real transactions** across 10 users, 8 categories, 30+ merchants
- **19 engineered features** per transaction (amount, velocity, geographic, temporal, categorical)
- **Hybrid fraud scoring** with weighted combination of 5 risk signals
- **Adaptive authentication** with meaningful questions about stores, locations, and categories (no risk scores)
- **Transaction habit prediction model** that learns each user's spending patterns
- **Production-ready RAG pipeline** with query rewriting, re-ranking, confidence scoring, and automated evaluation
- **Full pipeline monitoring** with execution logs and performance metrics

---

## System Architecture

![GraphGuard Architecture](docs/architecture_diagram.png)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CSV Data     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Ingestion    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Snowflake         ‚îÇ
‚îÇ (301 txns)   ‚îÇ     ‚îÇ Script       ‚îÇ     ‚îÇ RAW_TRANSACTIONS  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Feature Engineering                        ‚îÇ
‚îÇ  19 features: amount z-score, velocity (1h/24h/7d),         ‚îÇ
‚îÇ  category risk, geographic entropy, time-of-day, ‚Ä¶          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                     ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Fraud Model     ‚îÇ   ‚îÇ Auth Decision   ‚îÇ
          ‚îÇ IsolationForest ‚îÇ   ‚îÇ Risk Profiles   ‚îÇ
          ‚îÇ + Rule Scoring  ‚îÇ   ‚îÇ + Gemini LLM    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                     ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ        RAG Engine (ChromaDB)      ‚îÇ
          ‚îÇ  Embeds CSV + SQL ‚Üí Vector Store  ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Streamlit Dashboard      ‚îÇ
              ‚îÇ  üîë Auth  üö® Fraud       ‚îÇ
              ‚îÇ  üìä Dashboard  üîç RAG Q&A‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Gemini LLM (2.0 Flash)   ‚îÇ
              ‚îÇ  Dynamic Questions + RAG  ‚îÇ
              ‚îÇ  Answer Verification      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Monitoring & Logging     ‚îÇ
              ‚îÇ  pipeline_logs.csv        ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Pipeline Workflow

| Stage | Script / File | Input | Output |
|-------|--------------|-------|--------|
| **1. Ingestion** | `scripts/ingest_csv_to_snowflake.py` | CSV file | Snowflake `RAW_TRANSACTIONS` |
| **2. Feature Engineering** | `scripts/feature_engineering.py` | Raw transactions | `features_output.csv` (19 features) |
| **3. Fraud Scoring** | `models/fraud_model.py` | Feature CSV | `fraud_scores_output.csv` |
| **4. Auth Profiling** | `models/auth_decision.py` | Fraud scores | `auth_profiles_output.csv` |
| **5. Dashboard** | `streamlit_app.py` | All outputs | Interactive web app (Gemini + RAG) |

### Reproducing the Pipeline
```bash
# Step 1: Ingest data (dry-run mode ‚Äî no Snowflake credentials needed)
python scripts/ingest_csv_to_snowflake.py --dry-run

# Step 2: Compute features
python scripts/feature_engineering.py

# Step 3: Score transactions
python models/fraud_model.py

# Step 4: Generate auth profiles
python models/auth_decision.py

# Step 5: Launch dashboard
streamlit run streamlit_app.py
```

---

## Repository Structure

```
Hands-On-Week-4/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ CONTRIBUTIONS.md                   # Team member responsibilities
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ streamlit_app.py                   # Main Streamlit dashboard (Gemini + RAG)
‚îú‚îÄ‚îÄ pipeline_logs.csv                  # Pipeline execution log
‚îÇ
‚îú‚îÄ‚îÄ scripts/                          # Data pipeline scripts
‚îÇ   ‚îú‚îÄ‚îÄ ingest_csv_to_snowflake.py     # CSV ‚Üí Snowflake ingestion
‚îÇ   ‚îú‚îÄ‚îÄ ingest_config.yaml             # Connection configuration
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py         # Feature computation (19 features)
‚îÇ
‚îú‚îÄ‚îÄ sql/                              # SQL schemas & queries
‚îÇ   ‚îú‚îÄ‚îÄ create_tables.sql              # DDL for 5 Snowflake tables
‚îÇ   ‚îî‚îÄ‚îÄ analytical_queries.sql         # 8 analytical queries
‚îÇ
‚îú‚îÄ‚îÄ models/                           # Modeling / AI / Decision layer
‚îÇ   ‚îú‚îÄ‚îÄ fraud_model.py                 # Hybrid fraud scoring model
‚îÇ   ‚îú‚îÄ‚îÄ auth_decision.py               # Auth decision + Gemini integration
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py                  # Production RAG engine (ChromaDB + hybrid retrieval)
‚îÇ   ‚îú‚îÄ‚îÄ rag_evaluator.py               # Automated RAG accuracy evaluation (15 tests)
‚îÇ   ‚îú‚îÄ‚îÄ gemini_question_gen.py         # Dynamic question generation (Gemini LLM)
‚îÇ   ‚îî‚îÄ‚îÄ habit_model.py                 # Transaction habit prediction model
‚îÇ
‚îî‚îÄ‚îÄ docs/                             # Documentation assets
    ‚îî‚îÄ‚îÄ architecture_diagram.png       # System architecture diagram
```

---

## Quick Start

### Prerequisites
```bash
pip install -r requirements.txt
```

### Enable AI Features (Optional)
```bash
export GOOGLE_API_KEY="your-gemini-api-key-here"
# Or enter it in the Streamlit sidebar at runtime
```

### Run the Dashboard
```bash
streamlit run streamlit_app.py
# Open http://localhost:8501
```

### Run the Full Pipeline
```bash
# Feature engineering + scoring (no Snowflake needed)
python scripts/feature_engineering.py
python models/fraud_model.py
python models/auth_decision.py
```

---

## Component Details

### Data Ingestion (`scripts/`)
- Reads transaction CSV (301 rows, 10 users, 8 categories)
- Validates schema against expected columns
- Supports `--dry-run` mode for testing without Snowflake
- Logs every pipeline run to `pipeline_logs.csv`

### SQL Layer (`sql/`)
**5 tables** defined in `create_tables.sql`:
| Table | Purpose |
|-------|---------|
| `RAW_TRANSACTIONS` | Ingested transaction data |
| `TRANSACTION_FEATURES` | Computed features (19 columns) |
| `FRAUD_SCORES` | Model output risk scores |
| `AUTH_EVENTS` | Authentication event log |
| `PIPELINE_RUNS` | Pipeline execution metadata |

**8 analytical queries** in `analytical_queries.sql`:
User spending summaries, anomaly detection (>2œÉ), velocity checks, merchant risk profiles, geographic anomalies, daily trends, and category risk weights.

### Feature Engineering (`scripts/feature_engineering.py`)
Computes **19 features** per transaction:
- **Amount**: z-score, is_high_value flag
- **User-level**: avg/std spend, total transactions
- **Velocity**: transaction counts in 1h / 24h / 7d windows
- **Category**: risk weight (Jewelry=0.9, Coffee=0.05)
- **Time**: hour of day, day of week, is_weekend
- **Geographic**: is_new_location, location entropy

### Fraud Model (`models/fraud_model.py`)
Hybrid scoring with **5 weighted signals**:
| Signal | Weight | Method |
|--------|--------|--------|
| Z-Score Flag | 25% | Continuous scoring from amount deviation |
| Velocity Flag | 20% | Burst detection (1h, 24h thresholds) |
| Category Risk | 15% | Category-based risk weights |
| Geographic Risk | 15% | New location + location entropy |
| IsolationForest | 25% | Unsupervised anomaly detection |

Output: combined score 0.0‚Äì1.0 ‚Üí risk levels: LOW / MEDIUM / HIGH / CRITICAL

### Auth Decision (`models/auth_decision.py`)
- Computes user risk profiles from fraud scores
- Recommends security level and number of auth questions
- **New:** Integrates Gemini LLM for dynamic question generation via `generate_dynamic_questions()`
- **New:** LLM-powered answer verification via `verify_answers_with_llm()`

### RAG Engine (`models/rag_engine.py`)
- **Production-ready 5-step pipeline**: query rewriting ‚Üí metadata filtering ‚Üí expanded retrieval ‚Üí re-ranking ‚Üí confidence scoring
- Indexes project CSV outputs and SQL queries into **ChromaDB** vector store
- Uses **Google Generative AI embeddings** (`models/embedding-001`) when API key available
- **Aggregate document indexing**: category analysis, location heatmap, portfolio overview
- Provides `query()`, `get_context_for_user()`, `get_context_for_query()`, and `get_detailed_results()` methods

### RAG Evaluator (`models/rag_evaluator.py`)
- **15 ground truth test cases** covering user profiles, transactions, categories, locations, and portfolio queries
- **Retrieval metrics**: Hit Rate, MRR (Mean Reciprocal Rank), Type Match Rate, Average Latency
- **Answer quality scoring**: Gemini-judged accuracy, completeness, and readability (1-5 scale)
- Integrated into the Streamlit dashboard for one-click evaluation

### Gemini Question Generator (`models/gemini_question_gen.py`)
- **Dynamic security questions** about stores, locations, categories, and spending from the last 5 days
- **No risk scores** ‚Äî questions feel like a real bank verifying identity
- **30-second live countdown timer** per question (JavaScript-powered, runs independently)
- **Auto-replacement on miss** ‚Äî wrong answer or timeout generates a brand-new LLM question
- **Always unique** ‚Äî used-question tracking ensures no question is ever repeated
- Question difficulty scales with security level (LOW ‚Üí easy, CRITICAL ‚Üí very hard)
- **Structured output** with Key Findings, Analysis, Recommendations sections
- Falls back to static questions when no API key is available

### Transaction Habit Model (`models/habit_model.py`)
- Learns each user's transaction habits from historical data:
  - **Top 5 most visited stores** with visit count and average spend per store
  - **Preferred categories** ranked by frequency
  - **Typical locations** (top cities)
  - **Spending patterns** (avg/median amount, spending range)
  - **Time preferences** (peak hour, peak day, weekend vs weekday)
- **Habit consistency score** (0-100) measuring behavioral predictability
- **Next purchase prediction** based on historical frequency distribution
- **Similar user finder** using KNN on normalized spending features
- **Anomaly detection** ‚Äî checks if a new transaction matches learned habits

### Dashboard (`streamlit_app.py`)
| Tab | Function |
|-----|----------|
| üîë Authentication | Timed questions (30s countdown), auto-replacement on miss, always-unique LLM questions |
| üö® Fraud Detection | Per-transaction AI fraud analysis with RAG-enhanced structured explanations |
| üìä Dashboard | Risk distribution charts, user profiles, pipeline monitoring |
| üîç RAG Explorer | Free-form Q&A with confidence meter, source attribution, and RAG evaluation dashboard |
| üß† User Habits | Top 5 stores with avg spend, next purchase predictions, similar users, anomaly checker |

---

## Monitoring & Logging

### Pipeline Logs (`pipeline_logs.csv`)
Every pipeline execution is logged with:
```
run_id, timestamp, stage, status, records_processed, duration_ms, error_message
```

### Product Metrics (`logs/product_metrics.csv`)
Application-level metrics including:
- Authentication generation/verification events
- Fraud detection analysis events
- Latency measurements, confidence scores, success/failure status

---

## Implemented Extensions

1. **Hybrid ML + Rule-Based Model** ‚Äî Combines IsolationForest with interpretable rules
2. **Adaptive Authentication** ‚Äî Security level dynamically adjusts based on user risk profile
3. **Gemini LLM Question Generation** ‚Äî Meaningful questions about stores, locations, and categories (no risk scores)
4. **30-Second Live Timer** ‚Äî JavaScript-powered countdown per auth question with auto-expiry
5. **Auto-Replacement Questions** ‚Äî Wrong answer or timeout triggers a new unique LLM question (never repeats)
6. **Production RAG Pipeline** ‚Äî 5-step hybrid retrieval: query rewriting, metadata filtering, re-ranking, confidence scoring
7. **RAG Evaluation Suite** ‚Äî 15 ground truth tests with Hit Rate, MRR, Type Match, and LLM-judged answer quality
8. **Transaction Habit Prediction** ‚Äî Per-user habit learning with top 5 stores, KNN similarity, anomaly detection
9. **Top Stores with Avg Spend** ‚Äî Visual breakdown of most visited store categories with per-visit spending
10. **Structured AI Output** ‚Äî All AI answers formatted with Key Findings, Analysis, Recommendations, and Confidence
11. **RAG-Powered Q&A Explorer** ‚Äî Free-form natural-language queries with confidence meter and source attribution
12. **Full Pipeline Monitoring** ‚Äî Every stage logged with status, duration, and record counts
13. **Reproducible Pipeline** ‚Äî Config-driven, dry-run mode, documented step-by-step execution
14. **Geographic Anomaly Detection** ‚Äî Location entropy and new-location flagging
15. **Velocity-Based Detection** ‚Äî Multi-window (1h/24h/7d) transaction burst detection

---

## Demo Video

üìπ **[Demo Video Link]** ‚Äî *(To be added before submission)*

---

## Team

See [CONTRIBUTIONS.md](CONTRIBUTIONS.md) for detailed team member responsibilities.

**Course:** CS 5588 ‚Äî Data Science Capstone  
**Date:** February 2026
